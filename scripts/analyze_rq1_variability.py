"""RQ1 Analysis: Variability across IBM AI transpiler reasoning levels and seeds.

Research Question 1: How much structural and metric variability exists between
circuit variants generated by IBM's qiskit-ibm-transpiler across reasoning levels
and seeds, for representative NISQ-scale benchmarks?
"""

from __future__ import annotations

import argparse
import json
import sys
from pathlib import Path
from typing import Any, Dict, List

import pandas as pd

# Add project root to path to enable imports from benchmarks module
_project_root = Path(__file__).parent.parent
if str(_project_root) not in sys.path:
    sys.path.insert(0, str(_project_root))

from benchmarks.ai_transpile.statistics import (  # noqa: E402
    VarianceReport,
    compute_variance_analysis,
)
from benchmarks.ai_transpile.visualization import (  # noqa: E402
    plot_variance_boxplot_raw,
    setup_matplotlib_style,
)


def load_benchmark_results(results_file: Path) -> List[Dict[str, Any]]:
    """Load benchmark results from JSON file.

    Args:
        results_file: Path to latest_results.json

    Returns:
        List of result dictionaries
    """
    data = json.loads(results_file.read_text())
    return data["results"]


def filter_ai_transpiler_results(
    results: List[Dict[str, Any]],
    circuit_name: str | None = None,
) -> List[Dict[str, Any]]:
    """Filter results to only AI transpiler runs.

    Args:
        results: All benchmark results
        circuit_name: Optional circuit name to filter to

    Returns:
        Filtered results
    """
    filtered = [r for r in results if r["optimizer"] == "qiskit_ai" and r["metadata"]["variant"] == "ai_transpiler"]

    if circuit_name:
        filtered = [r for r in filtered if r["circuit"] == circuit_name]

    return filtered


def analyze_variability_by_level(
    results: List[Dict[str, Any]],
    metric: str = "two_qubit_gates",
) -> Dict[int, VarianceReport]:
    """Analyze variability within each optimization level.

    Args:
        results: AI transpiler results
        metric: Metric to analyze

    Returns:
        Dictionary mapping optimization level to VarianceReport
    """
    reports: Dict[int, VarianceReport] = {}

    for level in [1, 2, 3]:
        level_results = [r for r in results if r["metadata"]["optimization_level"] == level]

        if not level_results:
            continue

        values = [r["metrics"][metric] for r in level_results]
        report = compute_variance_analysis(values, metric_name=f"Level {level} - {metric}")
        reports[level] = report

    return reports


def create_variability_summary_table(
    reports: Dict[int, VarianceReport],
    output_dir: Path,
    latex: bool = False,
) -> pd.DataFrame:
    """Create summary table of variability across levels.

    Args:
        reports: Dictionary of VarianceReports by level
        output_dir: Output directory for table
        latex: Whether to export as LaTeX

    Returns:
        Summary DataFrame
    """
    rows = []

    for level, report in sorted(reports.items()):
        rows.append(
            {
                "Level": level,
                "Mean": f"{report.mean:.2f}",
                "Std Dev": f"{report.std:.2f}",
                "CV": f"{report.coefficient_of_variation:.4f}",
                "Min": f"{report.min_value:.0f}",
                "Max": f"{report.max_value:.0f}",
                "Range": f"{report.range:.0f}",
                "N": report.sample_size,
            }
        )

    df = pd.DataFrame(rows)

    output_file = output_dir / ("rq1_variability_table.tex" if latex else "rq1_variability_table.csv")
    output_dir.mkdir(parents=True, exist_ok=True)

    if latex:
        latex_str = df.to_latex(index=False, caption="RQ1: Variability Across Optimization Levels", label="tab:rq1")
        output_file.write_text(latex_str)
    else:
        df.to_csv(output_file, index=False)

    print(f"Saved variability table to {output_file}")

    return df


def create_per_circuit_analysis(
    results: List[Dict[str, Any]],
    output_dir: Path,
    metrics: List[str] = ["two_qubit_gates", "depth", "two_qubit_depth"],
) -> Dict[str, Dict[str, Dict[int, VarianceReport]]]:
    """Analyze variability for each circuit and metric.

    Args:
        results: All benchmark results
        output_dir: Output directory
        metrics: List of metrics to analyze

    Returns:
        Nested dictionary: {circuit: {metric: {level: VarianceReport}}}
    """
    circuits = sorted(set(r["circuit"] for r in results))
    analysis: Dict[str, Dict[str, Dict[int, VarianceReport]]] = {}

    for circuit in circuits:
        circuit_results = filter_ai_transpiler_results(results, circuit_name=circuit)

        if not circuit_results:
            continue

        analysis[circuit] = {}

        for metric in metrics:
            reports = analyze_variability_by_level(circuit_results, metric=metric)
            analysis[circuit][metric] = reports

            # Create visualization for this circuit-metric combination using raw data
            if circuit_results:
                fig_path = output_dir / f"rq1_{circuit}_{metric}_boxplot.pdf"
                try:
                    plot_variance_boxplot_raw(
                        circuit_results,
                        metric=metric,
                        group_by="optimization_level",
                        title=f"RQ1: {circuit} - {metric.replace('_', ' ').title()}",
                        output_path=fig_path,
                    )
                    print(f"Saved plot to {fig_path}")
                except Exception as e:
                    print(f"Warning: Could not create plot for {circuit}/{metric}: {e}")

    return analysis


def print_summary_statistics(analysis: Dict[str, Dict[str, Dict[int, VarianceReport]]]) -> None:
    """Print summary statistics to console.

    Args:
        analysis: Nested analysis results
    """
    print("\n" + "=" * 80)
    print("RQ1: VARIABILITY ANALYSIS SUMMARY")
    print("=" * 80)

    for circuit, metrics_data in analysis.items():
        print(f"\nCircuit: {circuit}")
        print("-" * 80)

        for metric, level_reports in metrics_data.items():
            print(f"\n  Metric: {metric}")

            for level, report in sorted(level_reports.items()):
                print(f"    Level {level}:")
                print(f"      Mean: {report.mean:.2f} ± {report.std:.2f}")
                print(f"      Range: [{report.min_value:.0f}, {report.max_value:.0f}]")
                print(f"      CV: {report.coefficient_of_variation:.4f}")
                print(f"      Samples: {report.sample_size}")


def main() -> None:
    """Main analysis function for RQ1."""
    parser = argparse.ArgumentParser(description="Analyze variability across AI transpiler runs (RQ1)")
    parser.add_argument(
        "--results",
        type=Path,
        default=Path("reports/circuit_benchmark/full/latest_results.json"),
        help="Path to benchmark results JSON",
    )
    parser.add_argument(
        "--output-dir",
        type=Path,
        default=Path("reports/paper_figures"),
        help="Output directory for figures and tables",
    )
    parser.add_argument(
        "--metrics",
        nargs="+",
        default=["two_qubit_gates", "depth", "two_qubit_depth"],
        help="Metrics to analyze",
    )
    parser.add_argument(
        "--latex",
        action="store_true",
        help="Export tables as LaTeX",
    )
    args = parser.parse_args()

    # Setup matplotlib
    setup_matplotlib_style()

    # Load results
    print(f"Loading results from {args.results}...")
    results = load_benchmark_results(args.results)
    print(f"Loaded {len(results)} total results")

    # Filter to AI transpiler results
    ai_results = filter_ai_transpiler_results(results)
    print(f"Found {len(ai_results)} AI transpiler results")

    if not ai_results:
        print("No AI transpiler results found. Exiting.")
        return

    # Perform analysis
    print("\nAnalyzing variability across circuits and metrics...")
    analysis = create_per_circuit_analysis(ai_results, args.output_dir, metrics=args.metrics)

    # Print summary
    print_summary_statistics(analysis)

    # Create overall summary for most important metric
    print(f"\nCreating overall summary table for {args.metrics[0]}...")
    primary_metric_reports = {}

    for level in [1, 2, 3]:
        level_results = [r for r in ai_results if r["metadata"]["optimization_level"] == level]
        if level_results:
            values = [r["metrics"][args.metrics[0]] for r in level_results]
            report = compute_variance_analysis(values, metric_name=f"Level {level}")
            primary_metric_reports[level] = report

    if primary_metric_reports:
        create_variability_summary_table(primary_metric_reports, args.output_dir, latex=args.latex)

    print(f"\n✓ RQ1 analysis complete. Results saved to {args.output_dir}")


if __name__ == "__main__":
    main()

